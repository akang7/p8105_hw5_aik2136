Homework 5
================
Ashley Kang

### Problem 1

##### Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

Start with a dataframe containing all file names; the `list.files` function will help. Iterate over file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe

``` r
df_1 =
  tibble(filenames = list.files("data", full.names = TRUE), map(filenames, read_csv)) %>% 
  unnest() %>% 
  separate(filenames, into = c("arm", "id"), sep = "_") %>%
  mutate(arm = str_remove(arm, "data/"), id = str_remove(id, ".csv")) %>% 
  mutate(arm = recode(arm, "'con' = 'Control'; 'exp' = 'Experimental'"))
```

Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary

``` r
tidy_df = df_1 %>% 
  gather(key = week, value = value, "week_1":"week_8") %>% 
  mutate(week = str_remove(week, "week_")) %>% 
  mutate(week = as.numeric(week))  %>%
  arrange(week, id)
```

##### Spaghetti plot

``` r
ggplot(tidy_df, aes(week, value, group = id, color = id)) +
  geom_line() +
  facet_grid(~ arm) +
  labs(title = "Observations of each subject over time by arm",
       x = "Week",
       y = "Observations") +
  viridis::scale_color_viridis(name = "ID", discrete = TRUE) +
  theme_bw() +
  theme(axis.title = element_text(size = 16), 
        axis.text = element_text(size = 14), 
        plot.title = element_text(size = 20), 
        strip.text.x = element_text(size = 14), 
        legend.text = element_text(size = 14)
        )
```

![](p8105_hw5_aik2136_files/figure-markdown_github/spaghetti_plot_1-1.png)

The spaghetti plot shows that individuals assigned to the control arm appear to remain constant across the study period while individuals assigned to the experimental arm appear to have increasing observation values. Originally, I had converted ID to a numeric variable, but my legend did not output the way I wanted it to look (gives a continuous bar legend if I coerce ID into a numeric variable), so I kept ID as a character variable.

### Problem 2

##### *The Washington Post* has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository.

Describe the raw data.

``` r
homicide_data = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  janitor::clean_names()
```

*The Washington Post* homicide dataset contains information on **52179 homicides** from 2007 to 2015 across 50 major U.S. cities. There are **12 variables**, which provide information on the the victim's name, age, sex, race, reported date of homicide, location of homicide, and whether or not the case was solved. The data seems to be tidy.

Create a `city_state` variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

``` r
grouped_homicide = homicide_data %>%
  unite("city_state", c("city", "state"), sep = ", ") %>% 
  mutate(disposition = recode(disposition, "'Closed without arrest' = 'unsolved'; 'Open/No arrest' = 'unsolved'; 'Closed by arrest' = 'solved'")) %>%
  mutate(city_state = recode(city_state, "'Tulsa, AL' = 'Tulsa, OK'")) %>% 
  group_by(city_state) %>% 
  summarize(total = n(), unsolved = sum(disposition == "unsolved"))
```

For some reason, there was one entry in which Tulsa, Oklahoma changed to Tulsa, AL after combining `city` and `state` to create `city_state`, so I recoded it from Tulsa, AL to Tulsa, OK.

For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

``` r
homicide_baltimore = grouped_homicide %>% 
  filter(city_state == 'Baltimore, MD')

prop_balitmore = broom::tidy(prop.test(homicide_baltimore$unsolved, homicide_baltimore$total))

knitr::kable(prop_balitmore, col.names = c("Estimate",
                           "Statistic",
                           "P-value",
                           "Parameter",
                           "Lower bound",
                           "Upper bound",
                           "Method",
                           "Alternative"))
```

|   Estimate|  Statistic|  P-value|  Parameter|  Lower bound|  Upper bound| Method                                               | Alternative |
|----------:|----------:|--------:|----------:|------------:|------------:|:-----------------------------------------------------|:------------|
|  0.6455607|    239.011|        0|          1|    0.6275625|    0.6631599| 1-sample proportions test with continuity correction | two.sided   |

The estimated proportion of unsolved cases in Baltimore, MD is 64.6% of all homicide cases (95% CI: 62.8%, 66.3%).

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of `purrr::map`, `purrr::map2`, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

``` r
prop_homicides = grouped_homicide %>% 
  mutate(homicides = map2(unsolved, total, ~broom::tidy(prop.test(.x, .y)))) %>% 
  unnest() %>% 
  select(city_state:estimate, conf.low:conf.high) %>% 
  janitor::clean_names()

knitr::kable(head(prop_homicides), col.names = c("City, State",
                           "Total",
                           "Unsolved",
                           "Estimate",
                           "Lower bound",
                           "Upper bound"))
```

| City, State     |  Total|  Unsolved|   Estimate|  Lower bound|  Upper bound|
|:----------------|------:|---------:|----------:|------------:|------------:|
| Albuquerque, NM |    378|       146|  0.3862434|    0.3372604|    0.4375766|
| Atlanta, GA     |    973|       373|  0.3833505|    0.3528119|    0.4148219|
| Baltimore, MD   |   2827|      1825|  0.6455607|    0.6275625|    0.6631599|
| Baton Rouge, LA |    424|       196|  0.4622642|    0.4141987|    0.5110240|
| Birmingham, AL  |    800|       347|  0.4337500|    0.3991889|    0.4689557|
| Boston, MA      |    614|       310|  0.5048860|    0.4646219|    0.5450881|

Create a plot that shows the estimates and CIs for each city – check out geom\_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

``` r
prop_homicides %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point(color = "red") + 
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + 
  coord_flip() + 
  labs(title = "Proportion of unsolved cases in 50 major US cities with 95% CI", 
       y = "Proportion of unsolved cases", 
       x = "Location",
       caption = "* Error bar indicates 95% confidence interval") + 
  theme_classic() +
  theme(axis.title = element_text(size = 16), 
        plot.title = element_text(size = 20)
        ) 
```

![](p8105_hw5_aik2136_files/figure-markdown_github/plot_2-1.png)

The resulting plot shows the estimated proportion of unsolved homicide cases in each of the 50 cities with a corresponding 95% confidence interval. Chicago has the highest proportion of unsolved cases (which is considerably higher than the rest of the cities), while Richmond has the lowest proportion of unsolved cases.
